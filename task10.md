

- 学习Gini指数

  **定义：**基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。

  ​        **注意**： Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。

  即 **基尼指数（基尼不纯度）= 样本被选中的概率 \* 样本被分错的概率**

  $\operatorname{Gini}(\mathrm{p})=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}$

- 学习回归树

  回归树是可以用于回归的决策树模型，一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值.与分类树不同的是，回归树对输入空间的划分采用一种启发式的方法，会遍历所有输入变量，找到最优的切分变量和最优的切分点。 

  一个输入空间的划分的误差是用真实值和划分区域的预测值的最小二乘来衡量的，即 

  $\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$

  梳理一下上述内容，最小二乘回归树的生成方法如下:

  1. 选择最优的切分变量jj和最优的切分点ss，求解 
     $\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]$

  遍历所有特征，对固定的特征扫描所有取值，找到使上式达到最小值的对(j,s)(j,s).

  2. 用选定的对 (j,s)(j,s)划分区域，并确定该区域的预测值；
  3. 继续对两个字区域调用上述步骤，直至满足停止条件；

  4. 生成回归树；

- 剪枝

  预剪枝

  后剪枝