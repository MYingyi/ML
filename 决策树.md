

决策树小结

**1.  ID3、C4.5、CART**

这三个是非常著名的决策树算法。简单粗暴来说:

ID3 使用**信息增益**作为选择特征的准则；

C4.5 使用**信息增益比**作为选择特征的准则；

CART 使用 **Gini 指数**作为选择特征的准则。

**2.  熵**

在信息论与概率论中，熵用于表示**随机变量不确定性的度量**。

设X是一个有限状态的离散型随机变量，其概率分布为:

![img](https://img-blog.csdn.net/20180808121329929?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1bmh1bnRpNDUyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 

则随机变量X的熵定义为：

![img](https://img-blog.csdn.net/20180808121347932?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1bmh1bnRpNDUyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

熵越大，则随机变量的不确定性越大。

**3. 条件熵(conditional entropy)**

随机变量X给定的条件下，随机变量Y的条件熵H(Y|X)定义为：

![img](https://img-blog.csdn.net/20180808121357594?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1bmh1bnRpNDUyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中，pi = P(X = xi)。

**4. 信息增益(information gain)**

信息增益表示的是：得知特征X的信息而使得类Y的信息的不确定性减少的程度。

特征A对训练数据集D的信息增益g(D,A)定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即

g(D,A)=H(D)-H(D|A)

一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息(mutual information).即：信息增益也称为互信息。

**ID3**

熵表示的是数据中包含的信息量大小(混乱度)。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。

信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性a来进行划分所获得的 “纯度提升” 越大 。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。

ID3 仅仅适用于**二分类问题**。ID3 仅仅能够处理离散属性。

**C4.5**

C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = **信息增益 / 划分前熵** **选择信息增益比最大的作为最优特征**。公式：

![img](https://img-blog.csdn.net/20180808121505489?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1bmh1bnRpNDUyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。

5. CART

CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。

CART 的全称是**分类与回归树**（classification and regression tree）。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。

**回归树**

使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。

要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。

具体为，假设已将输入空间划分为 M 个单元R1,R2,...,Rm，即 M 个特征，并且在每个单元Rm上有一个固定的输出值Cm，于是回归树可以表示为

![img](https://img-blog.csdn.net/20180808121529196?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1bmh1bnRpNDUyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



当输入空间的划分确定时，可以用平方误差![img](https://img-blog.csdn.net/2018080812154629?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1bmh1bnRpNDUyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优值。

6. 剪枝

   决策树是一种贪心算法，理想情况下的误差为0，但是容易过拟合。

   剪枝是为了防止这种情况的发生，主要有两种：预剪枝和后剪枝

   预剪枝坑内会导致书的生长不够，有欠拟合的风险，通常，对参数添加正则化是一种预剪枝的思想；

   而后剪枝是通过将树完全生长之后，再进行剪枝，更加准确，但是计算消耗大。

   另外，决策树是集成学习中常用的基分类器。